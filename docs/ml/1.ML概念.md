# 1.ML概念

#### 1.介绍一个最熟悉的机器学习算法

#### 2.机器学习导致误差的原因？过拟合、欠拟合对应的偏差和方差是怎样的？

- 偏差：模型无法表达数据集的复杂度，模型不够复杂，导致不能学习到基本关系，导致欠拟合；
- 方差：数据量有限，模型对数据过度敏感，导致方差；
- 过拟合容易导致高方差，欠拟合容易导致高偏差；

#### 3.如何解决过拟合问题？哪些角度

- 数据层面：更多数据、数据增强；
- 模型层面：更简单模型、更优化的模型；
- 正则化：权重衰减正则化；
- bagging等集成学习方法，深度学习中的dropout；
- 早停：训练集最优不一定在测试集最优
- BN、LN等实践中也有助于降低过拟合风险；

#### 5.说一下SVD怎么降维

SVD（奇异值分解）进行降维的基本思路是**保留矩阵中最重要的信息部分，同时丢弃那些次要的或冗余的信息，从而达到简化数据的目的**。下面我将详细介绍如何利用SVD来进行降维。

SVD是一种可以应用于任何大小矩阵的技术，它将一个矩阵分解为三个矩阵的乘积。对于一个m×n的矩阵A，它的SVD形式可以写为：

$$
[ A = U \Sigma V^T ]
$$

- $U$ ：是一个`m×m`的正交矩阵，包含了A的左奇异向量。
- $\Sigma$ ：是一个`m×n`的对角矩阵，对角线上的元素是A的奇异值，按从大到小排序。
- $V$ ：是一个`n×n`的正交矩阵，包含了A的右奇异向量。

降维的核心**在于选取**$\Sigma$\*\* 矩阵中的前k个最大的奇异值（k < n）\*\*，这样可以保留矩阵A最重要的k个方向的信息。通过这种方式，可以将原本的n维数据映射到k维空间，实现降维的目的。

假设我们要将矩阵A降维到k维空间：

1. **计算SVD**：首先计算矩阵A的SVD分解，得到( $A = U \Sigma V^T$ )。
2. **截断奇异值矩阵**：从($ \Sigma$ )矩阵中选取前k个最大的奇异值构成一个新的k阶对角矩阵( $\Sigma_k $)，这通常意味着保留( $\Sigma $)的前k行和前k列。
3. **选取主成分**：取( $U$ )矩阵的前k列构成新的矩阵( $U_k$ )，这实际上是从原始数据空间中选择了k个方向作为新的特征轴。
4. **重构矩阵**：利用( $U_k$ )和( $\Sigma_k$ )来重构一个近似矩阵( $A_{approx}$ )，这个矩阵是在k维空间内的最佳近似。有时也会直接使用( $U_k \Sigma_k$ )来表示降维后的数据，这样就可以得到一个m×k的矩阵，每个样本在这个新空间中的表示。
5. **降维后的应用**：降维后的数据可以用在多种场景中，如可视化、提高模型训练效率、减少过拟合风险等。

#### 6.推导softmax做激活函数求导
### Softmax 函数

Softmax 函数常用于多分类问题，它的公式如下：

对于一个输入向量 \( \mathbf{z} = [z_1, z_2, ..., z_n] \)，Softmax 函数的输出 \( \mathbf{y} = [y_1, y_2, ..., y_n] \) 为：

$$
y_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

其中：
- \( z_i \) 是输入向量的第 \( i \) 个元素，
- \( y_i \) 是输出向量的第 \( i \) 个元素。

### Softmax 激活函数的导数推导

为了计算 Softmax 函数的导数，我们需要计算每个输出 \( y_i \) 对输入 \( z_k \) 的偏导数。

#### 1. 对 \( y_i \) 相对于 \( z_k \) 的偏导数

##### 1.1 当 \( i = k \) 时的偏导数

假设我们计算的是第 \( i \) 个输出 \( y_i \) 对输入 \( z_i \) 的偏导数，首先我们从 Softmax 函数的定义开始：

$$
y_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

对 \( z_i \) 求导数时，利用商法则得到：

$$
\frac{\partial y_i}{\partial z_i} = \frac{\sum_{j=1}^{n} e^{z_j} \cdot e^{z_i} - e^{z_i} \cdot e^{z_i}}{(\sum_{j=1}^{n} e^{z_j})^2}
$$

可以简化为：

$$
\frac{\partial y_i}{\partial z_i} = y_i (1 - y_i)
$$

这是因为 \( y_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \)，所以 \( y_i (1 - y_i) \) 就是我们求导后的结果。

##### 1.2 当 \( i \neq k \) 时的偏导数

现在考虑第 \( i \) 个输出 \( y_i \) 对输入 \( z_k \)（\( k \neq i \)）的偏导数。我们同样从 Softmax 函数的定义出发，利用商法则：

$$
y_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

对 \( z_k \) 求导数时，得到：

$$
\frac{\partial y_i}{\partial z_k} = \frac{0 - e^{z_i} \cdot e^{z_k}}{(\sum_{j=1}^{n} e^{z_j})^2}
$$

可以简化为：

$$
\frac{\partial y_i}{\partial z_k} = - y_i y_k
$$

### 2. 总结 Softmax 的导数

Softmax 的导数可以写成以下的形式：

- 当 \( i = k \) 时：
  
  $$
  \frac{\partial y_i}{\partial z_i} = y_i (1 - y_i)
  $$

- 当 \( i \neq k \) 时：
  
  $$
  \frac{\partial y_i}{\partial z_k} = - y_i y_k
  $$

### 3. Softmax 导数的雅可比矩阵

我们可以将 Softmax 的导数表示为一个雅可比矩阵 \( J \)，其中每个元素是 \( \frac{\partial y_i}{\partial z_k} \)。对于一个 \( n \) 维的向量，雅可比矩阵的元素如下：

$$
J_{ik} = \frac{\partial y_i}{\partial z_k} = 
\begin{cases} 
y_i (1 - y_i) & \text{if } i = k \\
- y_i y_k & \text{if } i \neq k
\end{cases}
$$


#### 7.正则化项L1和L2为什么有用

- **L1正则化**，相当于为模型添加了一个先验知识，就是**权重矩阵W服从均值为0的拉普拉斯分布**；
- **L2正则化**，相当于为模型添加了一个正态分布先验知识，就是**权重矩阵W服从均值为0的正态分布**。
- **L1正则化和L2正则化可以看做是损失函数的惩罚项**。所谓『惩罚』是指对损失函数中的某些参数做一些限制。拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。

#### 8.L1正则不可导，如何优化

在不可导处无法进行梯度下降，此时采用坐标轴下降法：坐标轴下降法是沿着坐标轴的方向，每次固定m-1个数值，对最后一个数值求局部最优解，迭代m次（证明：凸函数在每一个维度都取得最小值，则此处就是全局最小值）；

#### 9.什么样的特征容易产生比较小的权重

特征的权重反映了该特征对于模型预测结果的重要性。较小的权重可能意味着该特征对于模型输出的影响较弱，可能是由于多种原因造成的。以下是一些可能导致特征产生较小权重的因素：

1. **特征的稀有性**：当一个特征在数据集中很少出现时，模型可能会给这个特征分配较小的权重。例如，在推荐系统中，如果某个用户只看过一次某种类型的电影，而其他用户都没有看过这种类型的电影，那么在训练模型时，这种类型的电影可能会被赋予较低的权重。
2. **特征的相关性**：如果一个特征与其他特征高度相关，那么模型可能会倾向于选择那些更具代表性的特征，并给其余相关特征分配较小的权重。这是因为高度相关的特征提供的信息可能是冗余的，因此不需要太高的权重来表达它们的重要性。
3. **特征的噪声水平**：如果一个特征包含大量的噪声，那么模型可能会自动调整权重以忽略这部分特征，从而减少噪声对预测结果的影响。这种情况下，特征的权重会变得较小。
4. **特征的无关性**：如果一个特征对于预测目标几乎没有贡献，那么模型会自然地给这个特征分配一个很小甚至接近于零的权重。这样的特征可能不会帮助模型提高预测准确率。
5. **模型的正则化**：使用正则化技术（如L1或L2正则化）可以帮助减少模型的复杂度并防止过拟合。在L1正则化下，模型倾向于将不重要的特征权重压缩至零，而在L2正则化下，不重要的特征权重可能会变得非常小。
6. **训练数据的数量**：在数据量较少的情况下，模型可能会对某些特征赋予较小的权重，因为缺乏足够的证据证明这些特征的重要性。
7. **特征的动态范围**：如果一个特征的动态范围（即最大值与最小值之间的差距）很小，那么这个特征的权重也可能会较小，因为即使在数值上发生改变，对模型输出的影响也不大。
8. **模型训练算法**：不同的训练算法可能对特征权重有不同的影响。例如，在使用随机梯度下降(SGD)进行模型更新时，相比批量更新(batch update)，更容易产生大量小权重的特征。

总之，特征的权重受到多种因素的影响，包括但不限于特征本身的质量、与目标变量的关系、数据集的特性以及所使用的模型训练方法等。为了更好地理解和优化模型，分析特征权重是非常有用的步骤之一。
